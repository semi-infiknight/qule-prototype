{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "from frechetdist import frdist\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from solver import Solver\n",
    "from data_loader import get_loader\n",
    "from torch.backends import cudnn\n",
    "from utils import *\n",
    "from models import Generator, Discriminator\n",
    "from data.sparse_molecular_dataset import SparseMolecularDataset\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/qiskit/providers/ibmq/ibmqfactory.py:109: UserWarning: Timestamps in IBMQ backend properties, jobs, and job results are all now in local time instead of UTC.\n",
      "  warnings.warn('Timestamps in IBMQ backend properties, jobs, and job results '\n"
     ]
    }
   ],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "qubits = 8\n",
    "# Set up your ibmq credentials first from https://quantum-computing.ibm.com/\n",
    "demo_on_ibmq = True\n",
    "\n",
    "if demo_on_ibmq:\n",
    "    dev = qml.device('qiskit.ibmq', wires=qubits, backend='ibmq_16_melbourne')\n",
    "else:\n",
    "    dev = qml.device('default.qubit', wires=qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def gen_circuit(w):\n",
    "    # random noise as generator input\n",
    "    z1 = random.uniform(-1, 1)\n",
    "    z2 = random.uniform(-1, 1)\n",
    "    layers = 1    \n",
    "    \n",
    "    # construct generator circuit for both atom vector and node matrix\n",
    "    for i in range(qubits):\n",
    "        qml.RY(np.arcsin(z1), wires=i)\n",
    "        qml.RZ(np.arcsin(z2), wires=i)\n",
    "        \n",
    "    for l in range(layers):\n",
    "        for i in range(qubits):\n",
    "            qml.RY(w[i], wires=i)\n",
    "        for i in range(qubits-1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "            qml.RZ(w[i+qubits], wires=i+1)\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, beta1=0.5, beta2=0.999, d_conv_dim=[[128, 64], 128, [128, 64]], d_lr=0.0001, d_repeat_num=6, dropout=0.0, g_conv_dim=[128], g_lr=0.0001, g_repeat_num=6, lambda_cls=1, lambda_gp=10, lambda_rec=10, layer=1, log_dir='qgan-hg/logs', log_step=10, lr_update_step=500, mode='train', model_save_dir='qgan-hg/models', model_save_step=1000, mol_data_dir='data/gdb9_9nodes.sparsedataset', n_critic=5, num_iters=5000, num_iters_decay=2500, num_workers=1, patches=1, post_method='softmax', quantum=True, qubits=8, result_dir='qgan-hg/results', resume_iters=None, sample_dir='qgan-hg/samples', sample_step=1000, test_iters=5000, use_tensorboard=False, z_dim=8)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Quantum circuit configuration\n",
    "parser.add_argument('--quantum', type=bool, default=True, help='choose to use quantum gan with hybrid generator')\n",
    "parser.add_argument('--patches', type=int, default=1, help='number of quantum circuit patches')\n",
    "parser.add_argument('--layer', type=int, default=1, help='number of repeated variational quantum layer')\n",
    "parser.add_argument('--qubits', type=int, default=8, help='number of qubits and dimension of domain labels')\n",
    "\n",
    "# Model configuration.\n",
    "parser.add_argument('--z_dim', type=int, default=8, help='dimension of domain labels')\n",
    "parser.add_argument('--g_conv_dim', default=[128], help='number of conv filters in the first layer of G')\n",
    "parser.add_argument('--d_conv_dim', type=int, default=[[128, 64], 128, [128, 64]], help='number of conv filters in the first layer of D')\n",
    "parser.add_argument('--g_repeat_num', type=int, default=6, help='number of residual blocks in G')\n",
    "parser.add_argument('--d_repeat_num', type=int, default=6, help='number of strided conv layers in D')\n",
    "parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
    "parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
    "parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
    "parser.add_argument('--post_method', type=str, default='softmax', choices=['softmax', 'soft_gumbel', 'hard_gumbel'])\n",
    "\n",
    "# Training configuration.\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='mini-batch size')\n",
    "parser.add_argument('--num_iters', type=int, default=5000, help='number of total iterations for training D')\n",
    "parser.add_argument('--num_iters_decay', type=int, default=2500, help='number of iterations for decaying lr')\n",
    "parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
    "parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='dropout rate')\n",
    "parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
    "\n",
    "# Test configuration.\n",
    "parser.add_argument('--test_iters', type=int, default=5000, help='test model from this step')\n",
    "\n",
    "# Miscellaneous.\n",
    "parser.add_argument('--num_workers', type=int, default=1)\n",
    "parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "parser.add_argument('--use_tensorboard', type=str2bool, default=False)\n",
    "\n",
    "# Directories.\n",
    "parser.add_argument('--mol_data_dir', type=str, default='data/gdb9_9nodes.sparsedataset')\n",
    "parser.add_argument('--log_dir', type=str, default='qgan-hg/logs')\n",
    "parser.add_argument('--model_save_dir', type=str, default='qgan-hg/models')\n",
    "parser.add_argument('--sample_dir', type=str, default='qgan-hg/samples')\n",
    "parser.add_argument('--result_dir', type=str, default='qgan-hg/results')\n",
    "\n",
    "# Step size.\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=1000)\n",
    "parser.add_argument('--model_save_step', type=int, default=1000)\n",
    "parser.add_argument('--lr_update_step', type=int, default=500)\n",
    "\n",
    "config = parser.parse_known_args()[0]\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.0, inplace=True)\n",
      "  )\n",
      "  (edges_layer): Linear(in_features=128, out_features=405, bias=True)\n",
      "  (nodes_layer): Linear(in_features=128, out_features=45, bias=True)\n",
      "  (dropoout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "G\n",
      "The number of parameters: 59202\n",
      "Discriminator(\n",
      "  (gcn_layer): GraphConvolution(\n",
      "    (linear1): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (agg_layer): GraphAggregation(\n",
      "    (sigmoid_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (tanh_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linear_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "D\n",
      "The number of parameters: 51777\n"
     ]
    }
   ],
   "source": [
    "self = Solver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained models from step 2580...\n",
      "IBM Q running job 4/16\r"
     ]
    }
   ],
   "source": [
    "# Inference of generated molecules\n",
    "start_iters = 0\n",
    "self.resume_iters = 2580\n",
    "\n",
    "if self.resume_iters:\n",
    "    start_iters = self.resume_iters\n",
    "    self.restore_model(self.resume_iters)\n",
    "    gen_weights = torch.tensor([-0.11443097,-0.23893048,-0.26079974,0.52572775,0.04154618,0.7797117,\\\n",
    "                                -0.22719051,0.04173521,-0.7405998,0.040963333,0.13625668,0.5491951,0.41576374,-0.059020802,0.7136884], requires_grad=True)\n",
    "ibm_sample_list = []\n",
    "for i in range(self.batch_size):\n",
    "    # Running time depends on the queue of IBM melbourne machine\n",
    "    if demo_on_ibmq:\n",
    "        print(\"IBM Q running job {}/{}\".format(i+1, self.batch_size), end=\"\\r\")\n",
    "    ibm_sample_list.append(gen_circuit(gen_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start inference.\n",
    "print('Start inference...')\n",
    "start_time = time.time()\n",
    "\n",
    "mols, _, _, a, x, _, _, _, _ = self.data.next_train_batch(self.batch_size)\n",
    "\n",
    "# =================================================================================== #\n",
    "#                             1. Preprocess input data                                #\n",
    "# =================================================================================== #\n",
    "\n",
    "a = torch.from_numpy(a).to(self.device).long()            # Adjacency.\n",
    "x = torch.from_numpy(x).to(self.device).long()            # Nodes.\n",
    "a_tensor = self.label2onehot(a, self.b_dim)\n",
    "x_tensor = self.label2onehot(x, self.m_dim)\n",
    "z = torch.stack(tuple(ibm_sample_list)).to(self.device).float()\n",
    "\n",
    "# Z-to-target\n",
    "edges_logits, nodes_logits = self.G(z)\n",
    "# Postprocess with Gumbel softmax\n",
    "(edges_hat, nodes_hat) = self.postprocess((edges_logits, nodes_logits), self.post_method)\n",
    "logits_fake, features_fake = self.D(edges_hat, None, nodes_hat)\n",
    "g_loss_fake = - torch.mean(logits_fake)\n",
    "\n",
    "# Real Reward\n",
    "rewardR = torch.from_numpy(self.reward(mols)).to(self.device)\n",
    "# Fake Reward\n",
    "(edges_hard, nodes_hard) = self.postprocess((edges_logits, nodes_logits), 'hard_gumbel')\n",
    "edges_hard, nodes_hard = torch.max(edges_hard, -1)[1], torch.max(nodes_hard, -1)[1]\n",
    "mols = [self.data.matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy(), strict=True)\n",
    "        for e_, n_ in zip(edges_hard, nodes_hard)]\n",
    "rewardF = torch.from_numpy(self.reward(mols)).to(self.device)\n",
    "\n",
    "# Value loss\n",
    "value_logit_real,_ = self.V(a_tensor, None, x_tensor, torch.sigmoid)\n",
    "value_logit_fake,_ = self.V(edges_hat, None, nodes_hat, torch.sigmoid)\n",
    "g_loss_value = torch.mean((value_logit_real - rewardR) ** 2 + (\n",
    "                           value_logit_fake - rewardF) ** 2)\n",
    "\n",
    "R=[list(a[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "F=[list(edges_hard[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "fd_bond_only = frdist(R, F)\n",
    "\n",
    "R=[list(x[i]) + list(a[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "F=[list(nodes_hard[i]) + list(edges_hard[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "fd_bond_atom = frdist(R, F)\n",
    "\n",
    "loss = {}\n",
    "loss['G/loss_fake'] = g_loss_fake.item()\n",
    "loss['G/loss_value'] = g_loss_value.item()\n",
    "loss['FD/fd_bond_only'] = fd_bond_only\n",
    "loss['FD/fd_bond_atom'] = fd_bond_atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print out training information.\n",
    "et = time.time() - start_time\n",
    "et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, start_iters, self.num_iters)\n",
    "\n",
    "# Log update\n",
    "m0, m1 = all_scores(mols, self.data, norm=True)     # 'mols' is output of Fake Reward\n",
    "m0 = {k: np.array(v)[np.nonzero(v)].mean() for k, v in m0.items()}\n",
    "m0.update(m1)\n",
    "loss.update(m0)\n",
    "for tag, value in loss.items():\n",
    "    log += \", {}: {:.4f}\".format(tag, value)\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only valid moleculues evaluated by RDKit\n",
    "valid_mols = [i for i in mols if i != None]\n",
    "\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "import matplotlib\n",
    "\n",
    "for mol in valid_mols:\n",
    "    AllChem.ComputeGasteigerCharges(mol)\n",
    "    contribs = [mol.GetAtomWithIdx(i).GetDoubleProp('_GasteigerCharge') for i in range(mol.GetNumAtoms())]\n",
    "    fig = SimilarityMaps.GetSimilarityMapFromWeights(mol, contribs, colorMap=None,  contourLines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kongsr-rdkit",
   "language": "python",
   "name": "kongsr-rdkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
